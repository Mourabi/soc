import os
import yaml
import numpy as np
import pandas as pd
from scipy.io import loadmat
from pathlib import Path
import tensorflow as tf
import matplotlib.pyplot as plt


# google drive connexion
from google.colab import drive
drive.mount('/content/drive')
# ========== CONFIGURATION ==========
mat_dir = "/content/drive/MyDrive/Battery_Data"  # <-- SET THIS IF DIFFIRENT
yml_path = "/content/drive/MyDrive/nasa_battery_metadata.yml"               # <-- SET THIS IF DIFFIRENT
csv_path = "all_batteries_discharge_cycles.csv"      # output file
NUM_POINTS = 128
MAX_CYCLES = 1000

# ========== 0. MATLAB STRUCT CONVERTER ==========
def mat_struct_to_dict(matobj):
    """Recursively convert scipy.io.loadmat mat_struct objects to nested dictionaries."""
    from scipy.io.matlab.mio5_params import mat_struct
    if isinstance(matobj, dict):
        return {k: mat_struct_to_dict(v) for k, v in matobj.items()}
    elif isinstance(matobj, mat_struct):
        out = {}
        for field in matobj._fieldnames:
            out[field] = mat_struct_to_dict(getattr(matobj, field))
        return out
    elif isinstance(matobj, np.ndarray):
        if matobj.dtype == object or matobj.dtype.kind == 'O':
            return [mat_struct_to_dict(el) for el in matobj]
        else:
            return matobj
    else:
        return matobj

# ========== 1. MAT -> CSV PREPROCESSING ==========
def anchor_resample(x, num_points):
    if len(x) == 0:
        return np.zeros(num_points)
    if len(x) == 1:
        return np.full(num_points, x[0])
    original_indices = np.linspace(0, len(x)-1, num=len(x))
    target_indices = np.linspace(0, len(x)-1, num=num_points)
    return np.interp(target_indices, original_indices, x)

def process_battery(battery_id, battery_meta, base_path, num_points):
    results = []
    mat_path = Path(base_path) / battery_meta['sub_dir'] / f"{battery_id}.mat"
    if not mat_path.exists():
        print(f"Warning: .mat file not found for {battery_id} at {mat_path}")
        return []
    try:
        mat_raw = loadmat(mat_path, struct_as_record=False, squeeze_me=True)
        mat_db = mat_struct_to_dict(mat_raw[battery_id])
    except Exception as e:
        print(f"Failed to load {mat_path}: {e}")
        return []
    # --- cycles field
    try:
        cycles = mat_db["cycle"]
        if isinstance(cycles, dict):
            cycles = [cycles]
    except Exception as e:
        print(f"No cycles found in {battery_id}: {e}")
        return []
    # --- loop through discharge cycles
    for i, c in enumerate(cycles):
        try:
            cyc_type = str(c.get("type", "")).lower()
        except Exception:
            continue
        if cyc_type != "discharge":
            continue
        # meta fields
        cycle_id = c.get("index", i)
        time_cycle_start = str(c.get("time", "N/A"))
        ambient_temperature = c.get("ambient_temperature", np.nan)
        dat = c.get("data", {})
        # Extract time series
        voltage = np.squeeze(dat.get("Voltage_measured", dat.get("voltage", np.array([]))))
        current = np.squeeze(dat.get("Current_measured", dat.get("current", np.array([]))))
        temperature = np.squeeze(dat.get("Temperature_measured", dat.get("temperature", np.array([]))))
        time = np.squeeze(dat.get("Time", dat.get("time", np.array([]))))
        cap_k = dat.get("Capacity", np.nan)
        try:
            cap_k = float(np.ravel(cap_k)[0])
        except Exception:
            cap_k = np.nan
        if len(voltage) < 5 or len(current) < 5:
            continue
        voltage_r = anchor_resample(voltage, num_points)
        current_r = anchor_resample(current, num_points)
        temperature_r = anchor_resample(temperature, num_points)
        time_r = anchor_resample(time, num_points)
        soc_r = 1.0 - time_r / (time_r[-1] if len(time_r) > 0 else 1.0)
        row = {
            'battery_id': battery_id,
            'cycle_id': cycle_id,
            'time_cycle_start': time_cycle_start,
            'ambient_temperature': ambient_temperature,
            'capacity_k': cap_k,
            'cap_0': battery_meta.get('c0', np.nan),
            'fade': battery_meta.get('fade_in_percent', np.nan),
            'cutoff_voltage': battery_meta.get('discharge', {}).get('cutoff_voltage', np.nan),
            'discharge_type': battery_meta.get('discharge', {}).get('discharge_type', ''),
            'discharge_amplitude': battery_meta.get('discharge', {}).get('discharge_amplitude', np.nan),
            'discharge_frequency': battery_meta.get('discharge', {}).get('discharge_frequency', np.nan),
            'discharge_dutycycle': battery_meta.get('discharge', {}).get('discharge_dutycycle', np.nan),
        }
        for j in range(num_points):
            row[f'voltage_{j}'] = voltage_r[j]
            row[f'current_{j}'] = current_r[j]
            row[f'temperature_{j}'] = temperature_r[j]
            row[f'time_{j}'] = time_r[j]
            row[f'soc_{j}'] = soc_r[j]
        results.append(row)
        if len(results) >= MAX_CYCLES:
            break
    return results

def preprocess_and_save_csv(mat_dir, yml_path, csv_path, num_points=128, max_cycles=1000):
    with open(yml_path, "r") as f:
        batteries = yaml.safe_load(f)
    all_data = []
    print("Extracting discharge cycles from .mat files ...")
    for battery_id, battery_meta in batteries.items():
        if not battery_meta.get("usable", True):
            continue
        data_rows = process_battery(battery_id, battery_meta, mat_dir, num_points)
        all_data.extend(data_rows)
        if len(all_data) >= max_cycles:
            break
    df = pd.DataFrame(all_data)
    df.to_csv(csv_path, index=False)
    print(f"Saved preprocessed CSV: {csv_path} [{len(df)} cycles]")

# ========== 2. ML PIPELINE ==========

def compute_minmax(cycles):
    mins, maxs = {}, {}
    for key in ['voltage', 'current', 'temperature']:
        all_vals = np.concatenate([cycle[key] for cycle in cycles])
        mins[key] = np.min(all_vals)
        maxs[key] = np.max(all_vals)
    return {k: (mins[k], maxs[k]) for k in mins}

# ======================================

def preprocess_cycle(cycle, stats, num_points):
    voltage = anchor_resample(cycle['voltage'], num_points)
    current = anchor_resample(cycle['current'], num_points)
    temperature = anchor_resample(cycle['temperature'], num_points)
    voltage = (voltage - stats['voltage'][0]) / (stats['voltage'][1] - stats['voltage'][0] + 1e-8)
    current = (current - stats['current'][0]) / (stats['current'][1] - stats['current'][0] + 1e-8)
    temperature = (temperature - stats['temperature'][0]) / (stats['temperature'][1] - stats['temperature'][0] + 1e-8)
    time = np.linspace(0, 1, num_points)
    features = np.stack([voltage, current, temperature, time], axis=1)
    return features

# ======================================

def simple_ecm(current, delta_t=1.0, capacity=2.0):
    soc = 1.0
    soc_profile = []
    for i in range(len(current)):
        soc -= (current[i] * delta_t) / (capacity * 3600)
        soc = np.clip(soc, 0.0, 1.0)
        soc_profile.append(soc)
    return np.array(soc_profile)

# ======================================

def load_cycles_from_csv(csv_path, num_points=128, max_cycles=1000):
    df = pd.read_csv(csv_path)
    all_cycles, all_socs = [], []
    for idx, row in df.iterrows():
        voltage = np.array([row[f'voltage_{j}'] for j in range(num_points)])
        current = np.array([row[f'current_{j}'] for j in range(num_points)])
        temperature = np.array([row[f'temperature_{j}'] for j in range(num_points)])
        soc = np.array([row[f'soc_{j}'] for j in range(num_points)])
        cycle = {'voltage': voltage, 'current': current, 'temperature': temperature}
        all_cycles.append(cycle)
        all_socs.append(soc)
        if len(all_cycles) >= max_cycles:
            break
    split_idx = int(0.8 * len(all_cycles))
    return (all_cycles[:split_idx], all_socs[:split_idx], all_cycles[split_idx:], all_socs[split_idx:])

# ======================================

def make_tf_dataset(cycles, socs, stats, num_points, batch_size=16):
    features, ecm_soc, true_soc = [], [], []
    for cycle, true_s in zip(cycles, socs):
        feats = preprocess_cycle(cycle, stats, num_points)
        ecm = simple_ecm(cycle['current'])
        ecm_r = anchor_resample(ecm, num_points)
        soc_r = anchor_resample(true_s, num_points)
        features.append(feats)
        ecm_soc.append(ecm_r)
        true_soc.append(soc_r)
    features = np.array(features).astype(np.float32)
    ecm_soc = np.array(ecm_soc).astype(np.float32)
    true_soc = np.array(true_soc).astype(np.float32)
    ds = tf.data.Dataset.from_tensor_slices((features, ecm_soc, true_soc))
    ds = ds.shuffle(buffer_size=128).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds

# ======================================

def build_pure_nn(input_dim):
    inputs = tf.keras.Input(shape=(None, input_dim))
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='relu'))(inputs)
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='relu'))(x)
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(x)
    model = tf.keras.Model(inputs, x)
    return model

# ======================================

def build_residual_nn(input_dim):
    inputs = tf.keras.Input(shape=(None, input_dim))
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='relu'))(inputs)
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='relu'))(x)
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(x)
    model = tf.keras.Model(inputs, x)
    return model

# ======================================

def build_lstm_nn(input_dim):
    inputs = tf.keras.Input(shape=(None, input_dim))
    x = tf.keras.layers.LSTM(64, return_sequences=True)(inputs)
    x = tf.keras.layers.LSTM(32, return_sequences=True)(x)
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(x)
    model = tf.keras.Model(inputs, x)
    return model

# ======================================

def build_transformer_nn(input_dim, num_heads=4, ff_dim=64, num_layers=2):
    inputs = tf.keras.Input(shape=(None, input_dim))
    x = tf.keras.layers.Dense(ff_dim)(inputs)
    for _ in range(num_layers):
        x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)
        attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x1, x1)
        x2 = tf.keras.layers.Add()([attn_output, x])
        x3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x2)
        ffn_output = tf.keras.layers.Dense(ff_dim, activation='relu')(x3)
        ffn_output = tf.keras.layers.Dense(ff_dim)(ffn_output)
        x = tf.keras.layers.Add()([ffn_output, x2])
    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(x)
    model = tf.keras.Model(inputs, x)
    return model

# ======================================

def mse_loss(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

# ======================================

def physics_informed_loss(y_true, y_pred, ecm_soc, alpha=0.1, beta=0.1):
    mse = tf.reduce_mean(tf.square(y_pred - y_true))
    coulomb = tf.reduce_mean(tf.square(y_pred - ecm_soc[..., None]))
    boundary = tf.reduce_mean(tf.nn.relu(y_pred - 1.0) + tf.nn.relu(0.0 - y_pred))
    return mse + alpha * coulomb + beta * boundary

# ======================================

def train_model(model, train_ds, val_ds, loss_type="mse", ecm_in=None, alpha=0.1, beta=0.1, epochs=20, lr=1e-3):
    optimizer = tf.keras.optimizers.Adam(lr)
    best_val_loss = np.inf
    train_loss_history = []
    val_loss_history = []
    for epoch in range(epochs):
        train_losses = []
        for features, ecm_soc, true_soc in train_ds:
            with tf.GradientTape() as tape:
                if loss_type == "mse":
                    pred = model(features, training=True)
                    pred_soc = ecm_soc[..., None] + pred if ecm_in else pred
                    loss = mse_loss(true_soc[..., None], pred_soc)
                elif loss_type == "physics":
                    pred = model(features, training=True)
                    pred_soc = ecm_soc[..., None] + pred
                    loss = physics_informed_loss(true_soc[..., None], pred_soc, ecm_soc, alpha, beta)
            grads = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))
            train_losses.append(loss.numpy())
        val_losses = []
        for features, ecm_soc, true_soc in val_ds:
            if loss_type == "mse":
                pred = model(features, training=False)
                pred_soc = ecm_soc[..., None] + pred if ecm_in else pred
                loss = mse_loss(true_soc[..., None], pred_soc)
            elif loss_type == "physics":
                pred = model(features, training=False)
                pred_soc = ecm_soc[..., None] + pred
                loss = physics_informed_loss(true_soc[..., None], pred_soc, ecm_soc, alpha, beta)
            val_losses.append(loss.numpy())
        avg_train_loss = np.mean(train_losses)
        avg_val_loss = np.mean(val_losses)
        train_loss_history.append(avg_train_loss)
        val_loss_history.append(avg_val_loss)
        print(f"[{loss_type}] Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            model.save_weights(f'best_{loss_type}_{model.name}.weights.h5')
    model.load_weights(f'best_{loss_type}_{model.name}.weights.h5')
    return train_loss_history, val_loss_history


# ======================================

def evaluate_model(model, val_ds, ecm_in=None):
    preds, trues = [], []
    for features, ecm_soc, true_soc in val_ds:
        pred = model(features, training=False)
        pred_soc = ecm_soc[..., None] + pred if ecm_in else pred
        preds.append(np.squeeze(pred_soc.numpy()))
        trues.append(np.squeeze(true_soc.numpy()))
    preds = np.concatenate(preds)
    trues = np.concatenate(trues)
    rmse = np.sqrt(np.mean((preds - trues) ** 2))
    mae = np.mean(np.abs(preds - trues))
    r2 = 1 - np.sum((preds - trues) ** 2) / np.sum((trues - np.mean(trues)) ** 2)
    return preds, trues, rmse, mae, r2

# ======================================

def ecm_baseline_eval(val_cycles, val_soc, num_points):
    preds, trues = [], []
    for cycle, soc in zip(val_cycles, val_soc):
        ecm = simple_ecm(cycle['current'])
        ecm_r = anchor_resample(ecm, num_points)
        soc_r = anchor_resample(soc, num_points)
        preds.append(ecm_r)
        trues.append(soc_r)
    preds = np.concatenate(preds)
    trues = np.concatenate(trues)
    rmse = np.sqrt(np.mean((preds - trues) ** 2))
    mae = np.mean(np.abs(preds - trues))
    r2 = 1 - np.sum((preds - trues) ** 2) / np.sum((trues - np.mean(trues)) ** 2)
    return preds, trues, rmse, mae, r2

# ======================================

def plot_multipanel_metrics(results):
    model_names = list(results.keys())
    rmse_vals = [results[k][2] for k in model_names]
    mae_vals = [results[k][3] for k in model_names]
    r2_vals  = [results[k][4] for k in model_names]
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    colors = ['gray', 'blue', 'green', 'orange', 'purple', 'cyan']
    axes[0].bar(model_names, rmse_vals, color=colors[:len(model_names)])
    axes[0].set_ylabel("RMSE")
    axes[0].set_title("RMSE Comparison")
    for i, v in enumerate(rmse_vals):
        axes[0].text(i, v, f"{v:.3f}", ha='center', va='bottom')
    axes[1].bar(model_names, mae_vals, color=colors[:len(model_names)])
    axes[1].set_ylabel("MAE")
    axes[1].set_title("MAE Comparison")
    for i, v in enumerate(mae_vals):
        axes[1].text(i, v, f"{v:.3f}", ha='center', va='bottom')
    axes[2].bar(model_names, r2_vals, color=colors[:len(model_names)])
    axes[2].set_ylabel("R²")
    axes[2].set_title("R² Comparison")
    for i, v in enumerate(r2_vals):
        axes[2].text(i, v, f"{v:.3f}", ha='center', va='bottom')
    plt.suptitle("Model Performance Comparison")
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

# ======================================

def plot_cycle_predictions(results, val_cycles, val_soc, num_samples=3):
    """
    Plot true vs. predicted SOC curves for a few random validation samples for each model.
    """
    import random
    sample_idxs = np.random.choice(len(val_cycles), size=num_samples, replace=False)
    model_names = list(results.keys())
    for idx in sample_idxs:
        plt.figure(figsize=(12, 6))
        t = np.arange(len(val_soc[idx]))
        plt.plot(t, val_soc[idx], label='True SOC', color='black', linewidth=2)
        for name in model_names:
            pred = results[name][0].reshape(-1, val_soc[idx].shape[0])[idx]
            plt.plot(t, pred, label=f'{name} Prediction', alpha=0.7)
        plt.title(f'Cycle {idx} - True vs. Predicted SOC')
        plt.xlabel('Time Step')
        plt.ylabel('SOC')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

# ======================================

def plot_error_histograms(results, val_soc):
    """
    Plot histogram of absolute errors for each model.
    """
    model_names = list(results.keys())
    plt.figure(figsize=(14, 7))
    for i, name in enumerate(model_names):
        preds = results[name][0]
        trues = results[name][1]
        errors = np.abs(preds - trues)
        plt.hist(errors, bins=50, alpha=0.5, label=f"{name} | Mean abs err: {np.mean(errors):.4f}")
    plt.title("Absolute Error Distributions")
    plt.xlabel("Absolute Error")
    plt.ylabel("Frequency")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# ======================================

def plot_correlation(results):
    """
    Scatter plot: predicted vs true SOC.
    """
    model_names = list(results.keys())
    plt.figure(figsize=(14, 7))
    for i, name in enumerate(model_names):
        preds = results[name][0]
        trues = results[name][1]
        plt.scatter(trues, preds, s=2, alpha=0.3, label=name)
    plt.plot([0, 1], [0, 1], '--', color='black')
    plt.title("Predicted vs. True SOC (All Models)")
    plt.xlabel("True SOC")
    plt.ylabel("Predicted SOC")
    plt.legend()
    plt.tight_layout()
    plt.show()

# ======================================

def save_results_csv(results, out_csv="model_comparison_results.csv"):
    rows = []
    for name, (preds, trues, rmse, mae, r2) in results.items():
        rows.append({'model': name, 'RMSE': rmse, 'MAE': mae, 'R2': r2})
    pd.DataFrame(rows).to_csv(out_csv, index=False)
    print(f"Saved summary metrics to: {out_csv}")


# ======================================

def plot_learning_curve(train_losses, val_losses, model_name="Model"):
    epochs = np.arange(1, len(train_losses)+1)
    plt.figure(figsize=(8,5))
    plt.plot(epochs, train_losses, label='Train Loss', marker='o')
    plt.plot(epochs, val_losses, label='Validation Loss', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'Learning Curve: {model_name}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# ======================================

def plot_multipanel_metrics(results):
    model_names = list(results.keys())
    rmse_vals = [results[k][2] for k in model_names]
    mae_vals = [results[k][3] for k in model_names]
    r2_vals  = [results[k][4] for k in model_names]

    fig, axes = plt.subplots(1, 3, figsize=(20, 6))
    colors = ['gray', 'blue', 'green', 'orange', 'purple', 'cyan']

    # RMSE
    axes[0].bar(model_names, rmse_vals, color=colors[:len(model_names)])
    axes[0].set_ylabel("RMSE", fontsize=16, fontweight='bold')
    axes[0].set_title("RMSE Comparison", fontsize=16, fontweight='bold')
    for i, v in enumerate(rmse_vals):
        axes[0].text(i, v + 0.01, f"{v:.3f}", ha='center', va='bottom', fontsize=13, fontweight='bold')

    # MAE
    axes[1].bar(model_names, mae_vals, color=colors[:len(model_names)])
    axes[1].set_ylabel("MAE", fontsize=16, fontweight='bold')
    axes[1].set_title("MAE Comparison", fontsize=16, fontweight='bold')
    for i, v in enumerate(mae_vals):
        axes[1].text(i, v + 0.01, f"{v:.3f}", ha='center', va='bottom', fontsize=13, fontweight='bold')

    # R²
    axes[2].bar(model_names, r2_vals, color=colors[:len(model_names)])
    axes[2].set_ylabel("R²", fontsize=16, fontweight='bold')
    axes[2].set_title("R² Comparison", fontsize=16, fontweight='bold')
    for i, v in enumerate(r2_vals):
        axes[2].text(i, v + 0.02, f"{v:.3f}", ha='center', va='bottom', fontsize=13, fontweight='bold')

    # X ticks font size
    for ax in axes:
        ax.set_xticklabels(model_names, fontsize=14, fontweight='bold', rotation=15)

    plt.suptitle("Model Performance Comparison", fontsize=18, fontweight='bold')
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

# ========== MAIN ==========

def main():
    # ------------ STEP 1: PREPROCESSING (.mat/.yml -> CSV) ------------
    print("STEP 1: Preprocessing all batteries to CSV...")
    preprocess_and_save_csv(
        mat_dir=mat_dir,
        yml_path=yml_path,
        csv_path=csv_path,
        num_points=NUM_POINTS,
        max_cycles=MAX_CYCLES
    )

    # ------------ STEP 2: LOAD CSV DATA FOR ML ------------
    print("\nSTEP 2: Loading cycles from CSV...")
    train_cycles, train_soc, val_cycles, val_soc = load_cycles_from_csv(
        csv_path,
        num_points=NUM_POINTS,
        max_cycles=MAX_CYCLES
    )
    print(f"Train cycles: {len(train_cycles)}, Validation cycles: {len(val_cycles)}")
    stats = compute_minmax(train_cycles + val_cycles)
    train_ds = make_tf_dataset(train_cycles, train_soc, stats, NUM_POINTS, batch_size=16)
    val_ds   = make_tf_dataset(val_cycles, val_soc, stats, NUM_POINTS, batch_size=16)
    results = {}
    loss_curves = {}  # Store training and validation losses

    # ------------ STEP 3: ECM BASELINE ------------
    print("\n[1/6] ECM Baseline:")
    ecm_preds, ecm_trues, ecm_rmse, ecm_mae, ecm_r2 = ecm_baseline_eval(val_cycles, val_soc, NUM_POINTS)
    results['ECM'] = (ecm_preds, ecm_trues, ecm_rmse, ecm_mae, ecm_r2)
    print(f"ECM Baseline: RMSE={ecm_rmse:.4f}, MAE={ecm_mae:.4f}, R2={ecm_r2:.4f}")

    # ------------ STEP 4: PURE NN ------------
    print("\n[2/6] Pure NN:")
    pure_nn = build_pure_nn(input_dim=4)
    pure_train_loss, pure_val_loss = train_model(pure_nn, train_ds, val_ds, loss_type="mse", ecm_in=False, epochs=20)
    pure_preds, pure_trues, pure_rmse, pure_mae, pure_r2 = evaluate_model(pure_nn, val_ds, ecm_in=False)
    results['Pure NN'] = (pure_preds, pure_trues, pure_rmse, pure_mae, pure_r2)
    loss_curves['Pure NN'] = (pure_train_loss, pure_val_loss)
    print(f"Pure NN: RMSE={pure_rmse:.4f}, MAE={pure_mae:.4f}, R2={pure_r2:.4f}")
    plot_learning_curve(pure_train_loss, pure_val_loss, model_name="Pure NN")

    # ------------ STEP 5: HYBRID NN (NO PHYSICS) ------------
    print("\n[3/6] Hybrid NN (No Physics):")
    hybrid_nn = build_residual_nn(input_dim=4)
    hybrid_train_loss, hybrid_val_loss = train_model(hybrid_nn, train_ds, val_ds, loss_type="mse", ecm_in=True, epochs=20)
    hybrid_preds, hybrid_trues, hybrid_rmse, hybrid_mae, hybrid_r2 = evaluate_model(hybrid_nn, val_ds, ecm_in=True)
    results['Hybrid NN'] = (hybrid_preds, hybrid_trues, hybrid_rmse, hybrid_mae, hybrid_r2)
    loss_curves['Hybrid NN'] = (hybrid_train_loss, hybrid_val_loss)
    print(f"Hybrid NN: RMSE={hybrid_rmse:.4f}, MAE={hybrid_mae:.4f}, R2={hybrid_r2:.4f}")
    plot_learning_curve(hybrid_train_loss, hybrid_val_loss, model_name="Hybrid NN")

    # ------------ STEP 6: HYBRID NN (PHYSICS-INFORMED) ------------
    print("\n[4/6] Hybrid+Physics Constraints:")
    hybrid_phys = build_residual_nn(input_dim=4)
    hybrid_phys_train_loss, hybrid_phys_val_loss = train_model(hybrid_phys, train_ds, val_ds, loss_type="physics", ecm_in=True, epochs=20)
    phys_preds, phys_trues, phys_rmse, phys_mae, phys_r2 = evaluate_model(hybrid_phys, val_ds, ecm_in=True)
    results['Hybrid+Physics'] = (phys_preds, phys_trues, phys_rmse, phys_mae, phys_r2)
    loss_curves['Hybrid+Physics'] = (hybrid_phys_train_loss, hybrid_phys_val_loss)
    print(f"Hybrid+Physics: RMSE={phys_rmse:.4f}, MAE={phys_mae:.4f}, R2={phys_r2:.4f}")
    plot_learning_curve(hybrid_phys_train_loss, hybrid_phys_val_loss, model_name="Hybrid+Physics")

    # ------------ STEP 7: LSTM HYBRID (NO CONSTRAINTS) ------------
    print("\n[5/6] LSTM Hybrid:")
    lstm_nn = build_lstm_nn(input_dim=4)
    lstm_train_loss, lstm_val_loss = train_model(lstm_nn, train_ds, val_ds, loss_type="mse", ecm_in=True, epochs=20)
    lstm_preds, lstm_trues, lstm_rmse, lstm_mae, lstm_r2 = evaluate_model(lstm_nn, val_ds, ecm_in=True)
    results['LSTM Hybrid'] = (lstm_preds, lstm_trues, lstm_rmse, lstm_mae, lstm_r2)
    loss_curves['LSTM Hybrid'] = (lstm_train_loss, lstm_val_loss)
    print(f"LSTM Hybrid: RMSE={lstm_rmse:.4f}, MAE={lstm_mae:.4f}, R2={lstm_r2:.4f}")
    plot_learning_curve(lstm_train_loss, lstm_val_loss, model_name="LSTM Hybrid")

    # ------------ STEP 8: TRANSFORMER HYBRID (NO CONSTRAINTS) ------------
    print("\n[6/6] Transformer Hybrid:")
    trans_nn = build_transformer_nn(input_dim=4)
    trans_train_loss, trans_val_loss = train_model(trans_nn, train_ds, val_ds, loss_type="mse", ecm_in=True, epochs=20)
    trans_preds, trans_trues, trans_rmse, trans_mae, trans_r2 = evaluate_model(trans_nn, val_ds, ecm_in=True)
    results['Transformer Hybrid'] = (trans_preds, trans_trues, trans_rmse, trans_mae, trans_r2)
    loss_curves['Transformer Hybrid'] = (trans_train_loss, trans_val_loss)
    print(f"Transformer Hybrid: RMSE={trans_rmse:.4f}, MAE={trans_mae:.4f}, R2={trans_r2:.4f}")
    plot_learning_curve(trans_train_loss, trans_val_loss, model_name="Transformer Hybrid")

    # ------------ STEP 9: MULTI-PANEL PLOTTING ------------
    plot_multipanel_metrics(results)

    # ------------ EXTRA ANALYSIS PLOTS ------------
    print("\nAdditional Visualizations:")
    plot_multipanel_metrics(results)
    plot_cycle_predictions(results, val_cycles, val_soc)
    plot_error_histograms(results, val_soc)
    plot_correlation(results)
    save_results_csv(results)

# ======================================

if __name__ == "__main__":
    main()
